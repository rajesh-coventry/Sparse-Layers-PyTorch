{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2337dca8",
   "metadata": {},
   "source": [
    "# **RAG Basics: My Book as Reference of my LLM Queries:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903fabc8",
   "metadata": {},
   "source": [
    "To make `my book` as an additional information source for my LLM (as inference for critical decisions), I have some options: \n",
    "\n",
    "1. I can train entirely new models with my custom embeddings and, everything custom (creating another DeepSeek for example) (*`Generally Very Difficult`*)\n",
    "\n",
    "2. I can fine-tune existing models on my book data (*`Requires Lot of Computation Power`*)\n",
    "\n",
    "3. I can create task-specific embeddings for similarity search\n",
    "\n",
    "4. I can use my book's content with pre-trained models via *`RAG`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df6aa7",
   "metadata": {},
   "source": [
    "##### *`What I want`: Create Embeddings of by Book:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d44b8d",
   "metadata": {},
   "source": [
    "I can't just create my own embeddings using either my own model or by using some embedding models out there and then use these embeddings with some large-language models like say Claude-4 then there is more than 99.9999% chance that, this approach will not work. \n",
    "\n",
    "There are multiple reasons for this: \n",
    "\n",
    "First, our choosen model (claude in this case) is propritory. It do not let use to tokenize and create embeddings using their embedders. We only have  permissions for input and output interfaces, all other things happens internally (like inside a blackbox). \n",
    "\n",
    "If we still want to use LLMs then instead of choosing propritory models like Claude we can try other opensource models which allows us to use their tokenizer and embedder for embeddings generation of our texts (like for example book). This generated embeddings will work for this choosen model only and not on other models. \n",
    "\n",
    "Another thing is that, we can't use these embeddings generated from a specific model directly unless we Fine-Tune the Base model with this embeddings generated or use it in downstream tasks like in RAGs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9c4516",
   "metadata": {},
   "source": [
    "**Use Purpose-Built Embedding Models:**  \n",
    "\n",
    "1. **Using State-of-the-Art Embedding Models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c993cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI's embedding models\n",
    "import openai\n",
    "embeddings = openai.Embedding.create(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    input=\"Your text here\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformers (open source)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode([\"Your text here\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d257d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohere embeddings\n",
    "import cohere\n",
    "co = cohere.Client('your-api-key')\n",
    "embeddings = co.embed(texts=[\"Your text here\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8d79d",
   "metadata": {},
   "source": [
    "These models are specifically trained to create good embeddings for tasks like:\n",
    "   - Similarity search\n",
    "   -  Clustering\n",
    "   - Retrieval\n",
    "   - Classification\n",
    "\n",
    "We can optimize the dimensions of our embeddings according to our needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different models for different needs: \n",
    "small_model = SentenceTransformer('all-MiniLM-L6-v2')  # 384 dimensions\n",
    "large_model = SentenceTransformer('all-mpnet-base-v2')  # 768 dimensions\n",
    "openai_model = \"text-embedding-3-large\"  # 3072 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d27b0a",
   "metadata": {},
   "source": [
    "Also, if we try to use propritory large language models for embeddings generations then, their (like Claude's or any other general Large Language Models) internal embeddings are optimized for language generation, not for similarity search or retrieval. But this is what we are trying to do with our book's contents: vectorize the text and embeed them and then use it as a reference in generating outputs from large language models (use as a context for large language models). Following code block explain what we want to do clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597afa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import anthropic\n",
    "\n",
    "class BookRAGSystem:\n",
    "    def __init__(self, book_text, anthropic_api_key):\n",
    "        \"\"\"\n",
    "        A system that combines specialized embedding models with Claude\n",
    "        for question-answering over your book content.\n",
    "        \"\"\"\n",
    "        # Step 1: Use a specialized embedding model (NOT Claude's internal ones)\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Step 2: Set up Claude for text generation\n",
    "        self.claude = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "        \n",
    "        # Step 3: Process your book\n",
    "        self.book_chunks = self._chunk_book(book_text)\n",
    "        self.book_embeddings = self._create_embeddings()\n",
    "    \n",
    "    def _chunk_book(self, book_text):\n",
    "        \"\"\"Split book into manageable chunks\"\"\"\n",
    "        # Simple chunking - you can make this more sophisticated\n",
    "        chunks = []\n",
    "        words = book_text.split()\n",
    "        chunk_size = 500  # words per chunk\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_embeddings(self):\n",
    "        \"\"\"Create embeddings for all book chunks\"\"\"\n",
    "        print(f\"Creating embeddings for {len(self.book_chunks)} chunks...\")\n",
    "        \n",
    "        # This uses the specialized embedding model\n",
    "        # NOT Claude's internal embeddings\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            self.book_chunks,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def find_relevant_chunks(self, query, top_k=3):\n",
    "        \"\"\"Find most relevant book chunks for a query\"\"\"\n",
    "        \n",
    "        # Create embedding for the query using the SAME model\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.book_embeddings)[0]\n",
    "        \n",
    "        # Get top-k most similar chunks\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        relevant_chunks = []\n",
    "        for idx in top_indices:\n",
    "            relevant_chunks.append({\n",
    "                'text': self.book_chunks[idx],\n",
    "                'similarity': similarities[idx]\n",
    "            })\n",
    "        \n",
    "        return relevant_chunks\n",
    "    \n",
    "    def answer_question(self, question):\n",
    "        \"\"\"Answer a question based on the book content\"\"\"\n",
    "        \n",
    "        # Step 1: Find relevant chunks using embedding similarity\n",
    "        relevant_chunks = self.find_relevant_chunks(question)\n",
    "        \n",
    "        # Step 2: Prepare context for Claude\n",
    "        context = \"\\n\\n\".join([chunk['text'] for chunk in relevant_chunks])\n",
    "        \n",
    "        # Step 3: Create prompt for Claude\n",
    "        prompt = f\"\"\"Based on the following text from a book:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please answer the question based on the provided text. If the answer isn't in the text, say so.\"\"\"\n",
    "        \n",
    "        # Step 4: Get response from Claude\n",
    "        response = self.claude.messages.create(\n",
    "            model=\"claude-3-sonnet-20240229\",\n",
    "            max_tokens=1000,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'answer': response.content[0].text,\n",
    "            'sources': relevant_chunks\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Your book content\n",
    "    book_text = \"\"\"\n",
    "    Your entire book content goes here...\n",
    "    This could be loaded from a file, database, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the system\n",
    "    rag_system = BookRAGSystem(book_text, \"your-anthropic-api-key\")\n",
    "    \n",
    "    # Ask questions about your book\n",
    "    question = \"What is the main theme of this book?\"\n",
    "    result = rag_system.answer_question(question)\n",
    "    \n",
    "    print(\"Answer:\", result['answer'])\n",
    "    print(\"\\nSources used:\")\n",
    "    for i, source in enumerate(result['sources']):\n",
    "        print(f\"{i+1}. Similarity: {source['similarity']:.3f}\")\n",
    "        print(f\"   Text: {source['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8e076",
   "metadata": {},
   "source": [
    "Here we used **Specialized embedding model**: Optimized for similarity search and retrieval\n",
    "\n",
    "Then use **Claude's language understanding**: Optimized for natural language generation and reasoning\n",
    "\n",
    "Also, creating embeddings is very cheap (or free with open-source models) and we use Claude only for the final text output generation. \n",
    "\n",
    "**Benefits of this approach:** \n",
    "   - You can experiment with different embedding models\n",
    "   - You can switch between different LLMs (Claude, GPT, etc.)\n",
    "   - You can optimize each component separately\n",
    "\n",
    "**The Key Insight:**\n",
    "**We don't need Claude's internal embeddings.** In fact, using specialized embedding models is often better because:\n",
    "   - **They're designed for embeddings**: Claude's embeddings are optimized for language generation\n",
    "   - **They're more efficient**: Smaller, faster, cheaper\n",
    "   - **They're accessible**: You can actually use them!\n",
    "   - **They're often better**: State-of-the-art embedding models often outperform LLM embeddings for similarity tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3b9b9",
   "metadata": {},
   "source": [
    "## **Some LLMs Provide Access to Tokenization and Embedding Layes:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d363599",
   "metadata": {},
   "source": [
    "*`Some LLMs DO Provide Access to Their Tokenization and Embeddings`*\n",
    "\n",
    "Some LLMs and organizations do provide access to their tokenization and embedding layers. Here are the key players:\n",
    "\n",
    "**1. OpenAI Models:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f113cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "# OpenAI's tokenizer (same one used by GPT models)\n",
    "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "tokens = tokenizer.encode(\"Hello world\")\n",
    "\n",
    "# OpenAI's embedding models (trained alongside their LLMs)\n",
    "embeddings = openai.Embedding.create(\n",
    "    model=\"text-embedding-3-large\",  # Same training as GPT models\n",
    "    input=\"Your text here\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de72fc",
   "metadata": {},
   "source": [
    "**2. Hugging Face Transformers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Use the exact same tokenizer and embeddings as any model\n",
    "model_name = \"microsoft/DialoGPT-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Get embeddings using the model's own embedding layer\n",
    "text = \"Hello world\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    embeddings = model.embeddings(tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4823d",
   "metadata": {},
   "source": [
    "**3. Google's Models:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f01de",
   "metadata": {},
   "source": [
    "```python\n",
    "# Google's PaLM API provides both tokenization and embeddings\n",
    "# Cohere also provides access to their tokenizer and embeddings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class OptimalEmbeddingPipeline:\n",
    "    \"\"\"\n",
    "    Uses the same tokenization and embedding approach as advanced LLMs\n",
    "    for maximum compatibility and performance otherwise they will work \n",
    "    fine. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key, target_model=\"gpt-4\"):\n",
    "        \"\"\"\n",
    "        Initialize with the SAME tokenizer and embedding model\n",
    "        that your target LLM uses.\n",
    "        \"\"\"\n",
    "        self.openai_client = openai.OpenAI(api_key=openai_api_key)\n",
    "        self.target_model = target_model\n",
    "        \n",
    "        # Use the EXACT same tokenizer as your target LLM\n",
    "        self.tokenizer = tiktoken.encoding_for_model(target_model)\n",
    "        \n",
    "        # Use embeddings from the same model family\n",
    "        self.embedding_model = \"text-embedding-3-large\"  # Same family as GPT-4\n",
    "        \n",
    "        print(f\"Initialized pipeline for {target_model}\")\n",
    "        print(f\"Using tokenizer: {self.tokenizer.name}\")\n",
    "        print(f\"Using embeddings: {self.embedding_model}\")\n",
    "    \n",
    "    def analyze_tokenization(self, text):\n",
    "        \"\"\"\n",
    "        Analyze how the LLM would tokenize your text\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        token_texts = [self.tokenizer.decode([token]) for token in tokens]\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'token_count': len(tokens),\n",
    "            'tokens': tokens,\n",
    "            'token_texts': token_texts\n",
    "        }\n",
    "    \n",
    "    def create_embeddings(self, texts):\n",
    "        \"\"\"\n",
    "        Create embeddings using the same approach as your target LLM\n",
    "        \"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Use the advanced embedding model from the same family\n",
    "        response = self.openai_client.embeddings.create(\n",
    "            model=self.embedding_model,\n",
    "            input=texts\n",
    "        )\n",
    "        \n",
    "        embeddings = []\n",
    "        for data in response.data:\n",
    "            embeddings.append(data.embedding)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def process_book_optimally(self, book_text):\n",
    "        \"\"\"\n",
    "        Process your book using the optimal strategy:\n",
    "        1. Same tokenization as target LLM\n",
    "        2. Same embedding approach as target LLM\n",
    "        3. Chunk sizes that work well with the target model\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Analyze tokenization patterns\n",
    "        sample_analysis = self.analyze_tokenization(book_text[:1000])\n",
    "        avg_tokens_per_word = sample_analysis['token_count'] / len(book_text[:1000].split())\n",
    "        \n",
    "        print(f\"Average tokens per word: {avg_tokens_per_word:.2f}\")\n",
    "        \n",
    "        # Step 2: Create optimal chunks based on token limits\n",
    "        target_tokens_per_chunk = 1000  # Good size for most LLMs\n",
    "        target_words_per_chunk = int(target_tokens_per_chunk / avg_tokens_per_word)\n",
    "        \n",
    "        chunks = self._chunk_text(book_text, target_words_per_chunk)\n",
    "        \n",
    "        # Step 3: Create embeddings using the same approach as target LLM\n",
    "        embeddings = self.create_embeddings(chunks)\n",
    "        \n",
    "        return {\n",
    "            'chunks': chunks,\n",
    "            'embeddings': embeddings,\n",
    "            'tokenization_info': {\n",
    "                'avg_tokens_per_word': avg_tokens_per_word,\n",
    "                'words_per_chunk': target_words_per_chunk,\n",
    "                'tokens_per_chunk': target_tokens_per_chunk\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _chunk_text(self, text, words_per_chunk):\n",
    "        \"\"\"Create chunks optimized for the target LLM\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), words_per_chunk):\n",
    "            chunk = ' '.join(words[i:i + words_per_chunk])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def find_relevant_content(self, query, book_data, top_k=3):\n",
    "        \"\"\"\n",
    "        Find relevant content using the same embedding space\n",
    "        as your target LLM\n",
    "        \"\"\"\n",
    "        # Create query embedding using the same model\n",
    "        query_embedding = self.create_embeddings([query])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, book_data['embeddings'])[0]\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'text': book_data['chunks'][idx],\n",
    "                'similarity': similarities[idx],\n",
    "                'tokens': len(self.tokenizer.encode(book_data['chunks'][idx]))\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_response(self, query, relevant_content):\n",
    "        \"\"\"\n",
    "        Generate response using the target LLM with optimal context\n",
    "        \"\"\"\n",
    "        # Prepare context that's optimized for the target LLM\n",
    "        context = \"\\n\\n\".join([item['text'] for item in relevant_content])\n",
    "        \n",
    "        # Count tokens to ensure we stay within limits\n",
    "        prompt = f\"Based on this text:\\n\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "        token_count = len(self.tokenizer.encode(prompt))\n",
    "        \n",
    "        print(f\"Prompt token count: {token_count}\")\n",
    "        \n",
    "        # Generate response using the target LLM\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=self.target_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on the provided text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'answer': response.choices[0].message.content,\n",
    "            'token_usage': {\n",
    "                'prompt_tokens': token_count,\n",
    "                'completion_tokens': response.usage.completion_tokens,\n",
    "                'total_tokens': response.usage.total_tokens\n",
    "            },\n",
    "            'sources': relevant_content\n",
    "        }\n",
    "\n",
    "# Example usage demonstrating the optimal approach\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize with your target LLM\n",
    "    pipeline = OptimalEmbeddingPipeline(\n",
    "        openai_api_key=\"your-api-key\",\n",
    "        target_model=\"gpt-4\"\n",
    "    )\n",
    "    \n",
    "    # Your book content\n",
    "    book_text = \"\"\"\n",
    "    Your book content here...\n",
    "    This will be processed using the same tokenization\n",
    "    and embedding approach as GPT-4 for optimal compatibility.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process book optimally\n",
    "    book_data = pipeline.process_book_optimally(book_text)\n",
    "    \n",
    "    # Query the book\n",
    "    query = \"What is the main theme?\"\n",
    "    relevant_content = pipeline.find_relevant_content(query, book_data)\n",
    "    \n",
    "    # Generate response using the target LLM\n",
    "    result = pipeline.generate_response(query, relevant_content)\n",
    "    \n",
    "    print(\"Answer:\", result['answer'])\n",
    "    print(\"Token usage:\", result['token_usage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8629e0f",
   "metadata": {},
   "source": [
    "### **Embeddings are for Retrieval, Not for the LLM Input:**\n",
    "\n",
    "We input in natural Languages into LLMs and not in the embeddings-form directly. This input-natural-language will be converted into compatible embeddings internally by the LLM system. \n",
    "\n",
    "Here's the crucial understanding: *`Our embeddings are used for finding relevant content from our book, not for feeding into Claude.`*\n",
    "\n",
    "**How the RAG System Actually Works:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import anthropic\n",
    "\n",
    "class HuggingFaceEmbeddingRAG:\n",
    "    \"\"\"\n",
    "    RAG system that uses HuggingFace embeddings for retrieval\n",
    "    and Claude for generation. This is a common and effective pattern.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, book_text, anthropic_api_key):\n",
    "        # Step 1: Use HuggingFace embedding model for retrieval\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(f\"Using embedding model: {self.embedding_model}\")\n",
    "        \n",
    "        # Step 2: Use Claude for generation\n",
    "        self.claude = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "        print(\"Using Claude for text generation\")\n",
    "        \n",
    "        # Step 3: Process the book\n",
    "        self.book_chunks = self._chunk_book(book_text)\n",
    "        self.book_embeddings = self._create_embeddings()\n",
    "        \n",
    "        print(f\"Created {len(self.book_chunks)} chunks with embeddings\")\n",
    "        \n",
    "    def _chunk_book(self, book_text):\n",
    "        \"\"\"Split book into chunks for retrieval\"\"\"\n",
    "        chunks = []\n",
    "        words = book_text.split()\n",
    "        chunk_size = 300  # Optimal size for retrieval\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_embeddings(self):\n",
    "        \"\"\"Create embeddings using HuggingFace model\"\"\"\n",
    "        print(\"Creating embeddings for book chunks...\")\n",
    "        \n",
    "        # This uses HuggingFace model - completely separate from Claude\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            self.book_chunks,\n",
    "            show_progress_bar=True,\n",
    "            batch_size=32\n",
    "        )\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def retrieve_relevant_chunks(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        STEP 1: Use HuggingFace embeddings to find relevant chunks\n",
    "        This is the RETRIEVAL part of RAG\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving relevant chunks for: '{query}'\")\n",
    "        \n",
    "        # Create embedding for query using the SAME HuggingFace model\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        # Calculate similarities with book embeddings\n",
    "        similarities = cosine_similarity(query_embedding, self.book_embeddings)[0]\n",
    "        \n",
    "        # Get top-k most similar chunks\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        relevant_chunks = []\n",
    "        for idx in top_indices:\n",
    "            relevant_chunks.append({\n",
    "                'text': self.book_chunks[idx],\n",
    "                'similarity': similarities[idx],\n",
    "                'chunk_id': idx\n",
    "            })\n",
    "        \n",
    "        print(f\"Found {len(relevant_chunks)} relevant chunks\")\n",
    "        return relevant_chunks\n",
    "    \n",
    "    def generate_answer(self, query, relevant_chunks):\n",
    "        \"\"\"\n",
    "        STEP 2: Send TEXT (not embeddings) to Claude for generation\n",
    "        This is the GENERATION part of RAG\n",
    "        \"\"\"\n",
    "        print(\"Generating answer using Claude...\")\n",
    "        \n",
    "        # Prepare context as TEXT for Claude\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Chunk {chunk['chunk_id']}]: {chunk['text']}\"\n",
    "            for chunk in relevant_chunks\n",
    "        ])\n",
    "        \n",
    "        # Create prompt with TEXT context\n",
    "        prompt = f\"\"\"Based on the following excerpts from a book:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a comprehensive answer based on the information in the excerpts above. If the information is not sufficient to answer the question, please say so.\"\"\"\n",
    "        \n",
    "        # Send TEXT to Claude (NOT embeddings)\n",
    "        response = self.claude.messages.create(\n",
    "            model=\"claude-3-sonnet-20240229\",\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'answer': response.content[0].text,\n",
    "            'context_used': context,\n",
    "            'sources': relevant_chunks\n",
    "        }\n",
    "    \n",
    "    def ask_question(self, query):\n",
    "        \"\"\"\n",
    "        Complete RAG pipeline:\n",
    "        1. Retrieve using HuggingFace embeddings\n",
    "        2. Generate using Claude\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== RAG Query: {query} ===\")\n",
    "        \n",
    "        # Step 1: Retrieve relevant chunks using HuggingFace embeddings\n",
    "        relevant_chunks = self.retrieve_relevant_chunks(query, top_k=3)\n",
    "        \n",
    "        # Step 2: Generate answer using Claude with retrieved TEXT\n",
    "        result = self.generate_answer(query, relevant_chunks)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def explain_workflow(self):\n",
    "        \"\"\"Explain how the system works\"\"\"\n",
    "        return \"\"\"\n",
    "        RAG WORKFLOW EXPLANATION:\n",
    "        \n",
    "        1. INDEXING PHASE (Done Once):\n",
    "           - Book text → Split into chunks\n",
    "           - Chunks → HuggingFace embeddings → Store in vector database\n",
    "           \n",
    "        2. QUERY PHASE (For Each Question):\n",
    "           - User question → HuggingFace embedding (same model)\n",
    "           - Find similar chunks using cosine similarity\n",
    "           - Retrieved chunks (as TEXT) → Claude\n",
    "           - Claude generates answer based on TEXT context\n",
    "           \n",
    "        KEY INSIGHT: \n",
    "        - HuggingFace embeddings: Used for RETRIEVAL only\n",
    "        - Claude: Receives TEXT, not embeddings\n",
    "        - No compatibility issues because Claude never sees the embeddings!\n",
    "        \"\"\"\n",
    "\n",
    "# Advanced example with multiple embedding models\n",
    "class MultiEmbeddingRAG:\n",
    "    \"\"\"\n",
    "    Advanced RAG that can use different embedding models\n",
    "    while still using Claude for generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, book_text, anthropic_api_key, embedding_model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        self.claude = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "        \n",
    "        print(f\"Embedding model: {embedding_model_name}\")\n",
    "        print(f\"Embedding dimensions: {self.embedding_model.get_sentence_embedding_dimension()}\")\n",
    "        \n",
    "        self.book_chunks = self._chunk_book(book_text)\n",
    "        self.book_embeddings = self._create_embeddings()\n",
    "    \n",
    "    def _chunk_book(self, book_text):\n",
    "        \"\"\"Smart chunking based on embedding model capabilities\"\"\"\n",
    "        chunks = []\n",
    "        sentences = book_text.split('.')\n",
    "        \n",
    "        current_chunk = \"\"\n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) < 500:  # Optimal for most models\n",
    "                current_chunk += sentence + \". \"\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \". \"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_embeddings(self):\n",
    "        \"\"\"Create embeddings with the chosen model\"\"\"\n",
    "        return self.embedding_model.encode(\n",
    "            self.book_chunks,\n",
    "            show_progress_bar=True,\n",
    "            normalize_embeddings=True  # Better for similarity search\n",
    "        )\n",
    "    \n",
    "    def compare_embedding_models(self, query):\n",
    "        \"\"\"Compare different embedding models for the same query\"\"\"\n",
    "        models_to_test = [\n",
    "            'all-MiniLM-L6-v2',\n",
    "            'all-mpnet-base-v2',\n",
    "            'paraphrase-MiniLM-L6-v2'\n",
    "        ]\n",
    "        \n",
    "        results = {}\n",
    "        for model_name in models_to_test:\n",
    "            temp_model = SentenceTransformer(model_name)\n",
    "            query_embedding = temp_model.encode([query])\n",
    "            temp_book_embeddings = temp_model.encode(self.book_chunks)\n",
    "            \n",
    "            similarities = cosine_similarity(query_embedding, temp_book_embeddings)[0]\n",
    "            top_idx = np.argmax(similarities)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'best_match': self.book_chunks[top_idx][:100] + \"...\",\n",
    "                'similarity': similarities[top_idx],\n",
    "                'dimensions': temp_model.get_sentence_embedding_dimension()\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    book_text = \"\"\"\n",
    "    Your book content here. This could be a novel, textbook, \n",
    "    research paper, or any text you want to query.\n",
    "    The system will work regardless of which HuggingFace \n",
    "    embedding model you choose.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    rag = HuggingFaceEmbeddingRAG(book_text, \"your-anthropic-api-key\")\n",
    "    \n",
    "    # Ask questions\n",
    "    questions = [\n",
    "        \"What is the main theme of this book?\",\n",
    "        \"Can you summarize the key concepts?\",\n",
    "        \"What are the practical applications mentioned?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        result = rag.ask_question(question)\n",
    "        print(f\"\\nQ: {question}\")\n",
    "        print(f\"A: {result['answer']}\")\n",
    "        print(f\"Sources used: {len(result['sources'])} chunks\")\n",
    "    \n",
    "    # Explain the workflow\n",
    "    print(rag.explain_workflow())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b43e54",
   "metadata": {},
   "source": [
    "**The Two-Stage Process:**\n",
    "   1. **`Retrieval Stage`**: HuggingFace embeddings find relevant content\n",
    "   2. **`Generation Stage`**: Claude receives TEXT (not embeddings) and generates answers\n",
    "\n",
    "**No Compatibility Issues:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423c6b7",
   "metadata": {},
   "source": [
    "```python\n",
    "# This is what happens:\n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "# Stage 1: Retrieval (HuggingFace)\n",
    "query_embedding = hf_model.encode([query])  # [0.1, 0.2, 0.3, ...]\n",
    "similar_chunks = find_similar(query_embedding, book_embeddings)\n",
    "\n",
    "# Stage 2: Generation (Claude)\n",
    "context = \"Machine learning is a subset of AI...\" # TEXT, not embeddings\n",
    "response = claude.generate(f\"Based on: {context}\\nQuestion: {query}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ceb57",
   "metadata": {},
   "source": [
    "#### **Common Embedding Models for RAG:**\n",
    "\n",
    "```python\n",
    "# Popular choices for book/document RAG:\n",
    "\n",
    "# Lightweight and fast\n",
    "\"all-MiniLM-L6-v2\"          # 384 dimensions, 22MB\n",
    "\n",
    "# Better quality\n",
    "\"all-mpnet-base-v2\"         # 768 dimensions, 420MB\n",
    "\n",
    "# Multilingual\n",
    "\"paraphrase-multilingual-MiniLM-L12-v2\"  # 384 dimensions\n",
    "\n",
    "# Domain-specific\n",
    "\"allenai/specter\"           # Scientific papers\n",
    "\"sentence-transformers/msmarco-distilbert-base-v4\"  # Web search\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32d011",
   "metadata": {},
   "source": [
    "#### **The Complete Workflow:**\n",
    "\n",
    "1. **Preprocessing** (Once):\n",
    "   ```raw\n",
    "      Book → Chunks → HuggingFace Embeddings → Vector Database\n",
    "   ```\n",
    "\n",
    "2. **Query Processing** (Each time):\n",
    "   ```raw\n",
    "      Query → HuggingFace Embedding → Similarity Search → Relevant Chunks\n",
    "   ```\n",
    "\n",
    "3. **Generation** (Each time):\n",
    "   ```raw\n",
    "      Relevant Chunks (as text) → Claude → Answer\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa22de68",
   "metadata": {},
   "source": [
    "#### **Key Advantages of This Approach:**\n",
    "\n",
    "**1. No Embedding Compatibility Issues:**\n",
    "   - LLMs (eg. Claude) never sees the embeddings\n",
    "   - Only sees the retrieved text chunks\n",
    "   - Works with any embedding model\n",
    "\n",
    "**2. Optimal Performance:**\n",
    "   - Best retrieval models for finding relevant content\n",
    "   - Best generation model for creating answers\n",
    "   - Each component does what it's best at\n",
    "\n",
    "**3. Easy to Optimize:**\n",
    "   - Can test different embedding models independently\n",
    "   - Can switch between different LLMs for generation\n",
    "   - Can tune retrieval and generation separately\n",
    "\n",
    "**4. Scalable:**\n",
    "   - Embedding computation can be done offline\n",
    "   - Only generation requires API calls\n",
    "   - Can cache embeddings for reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40af076",
   "metadata": {},
   "source": [
    "#### **Real-World Example:**\n",
    "\n",
    "```python\n",
    "# This is how major RAG systems actually work:\n",
    "\n",
    "# Retrieval: Specialized embedding models\n",
    "retriever = SentenceTransformer('all-mpnet-base-v2')\n",
    "relevant_docs = retriever.retrieve(query, document_store)\n",
    "\n",
    "# Generation: Powerful LLMs\n",
    "generator = AnthropicClaude()  # or OpenAI GPT, etc.\n",
    "answer = generator.generate(query, relevant_docs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdf1dd",
   "metadata": {},
   "source": [
    "### **The Complete RAG Pipeline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43758b2b",
   "metadata": {},
   "source": [
    "**For our book example:**  \n",
    "\n",
    "1. **Book chunks along with its generated embeddings are STORED as text in the vector database (in our computer)** (not just embeddings)\n",
    "\n",
    "2. **Embeddings of book-contents are used ONLY for similarity comparison between our input query and the book contents**\n",
    "\n",
    "3. **Retrieved chunks are already in natural language because we use similarity to compare the vectors between our query vector and the book-embeddings stored on the databases and matched chunks (actual text) will be retrieved instead of the numeric embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a734934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our vector store actually looks like this:\n",
    "\n",
    "vector_store = {\n",
    "    'chunk_0': {\n",
    "        'text': \"Machine learning is a subset of AI...\",  # Natural language\n",
    "        'embedding': [0.1, 0.2, 0.3, ...]  # Vector for similarity search\n",
    "    },\n",
    "    'chunk_1': {\n",
    "        'text': \"There are three main types...\",  # Natural language\n",
    "        'embedding': [0.4, 0.5, 0.6, ...]  # Vector for similarity search\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f448fb7e",
   "metadata": {},
   "source": [
    "**The Search Process:**\n",
    "   1. Query → Embedding: `[0.2, 0.3, 0.4, ...]`\n",
    "   2. Find similar embeddings: `chunk_0.embedding` matches best\n",
    "   3. Return the TEXT: `\"Machine learning is a subset of AI...\"`\n",
    "   4. Send TEXT to Claude\n",
    "\n",
    "**Who Handles What - Simple Summary**\n",
    "1. **Our RAG Application:**\n",
    "   - ✅ Loads HuggingFace embedding model\n",
    "   - ✅ Converts text to embeddings\n",
    "   - ✅ Stores both text AND embeddings\n",
    "   - ✅ Searches embeddings for similarity\n",
    "   - ✅ Retrieves corresponding text chunks\n",
    "   - ✅ Sends text to Claude\n",
    "\n",
    "2. **Choice (Claude):**\n",
    "   - ✅ Receives text prompt\n",
    "   - ✅ Generates text response\n",
    "   - ❌ Never sees embeddings\n",
    "   - ❌ Never does vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import anthropic\n",
    "\n",
    "class RAGPipelineBreakdown:\n",
    "    \"\"\"\n",
    "    Detailed breakdown of WHO handles WHAT in the RAG pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, book_text, anthropic_api_key):\n",
    "        print(\"=== INITIALIZING RAG SYSTEM ===\")\n",
    "        \n",
    "        # YOUR APPLICATION handles embedding model\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"✓ Your App: Loaded HuggingFace embedding model\")\n",
    "        \n",
    "        # YOUR APPLICATION handles Claude client\n",
    "        self.claude = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "        print(\"✓ Your App: Connected to Claude API\")\n",
    "        \n",
    "        # YOUR APPLICATION handles book processing\n",
    "        self.book_chunks, self.book_embeddings = self._process_book(book_text)\n",
    "        print(f\"✓ Your App: Created {len(self.book_chunks)} chunks with embeddings\")\n",
    "        \n",
    "        print(\"=== RAG SYSTEM READY ===\\n\")\n",
    "    \n",
    "    def _process_book(self, book_text):\n",
    "        \"\"\"YOUR APPLICATION handles all book processing\"\"\"\n",
    "        print(\"--- Book Processing (Your App) ---\")\n",
    "        \n",
    "        # Step 1: YOUR APP splits text into chunks\n",
    "        chunks = self._chunk_text(book_text)\n",
    "        print(f\"✓ Your App: Split book into {len(chunks)} chunks\")\n",
    "        \n",
    "        # Step 2: YOUR APP creates embeddings using HuggingFace\n",
    "        embeddings = self.embedding_model.encode(chunks, show_progress_bar=True)\n",
    "        print(f\"✓ Your App: Created embeddings (shape: {embeddings.shape})\")\n",
    "        \n",
    "        # Step 3: YOUR APP stores embeddings (in memory/disk/database)\n",
    "        # This is your vector store!\n",
    "        print(\"✓ Your App: Stored embeddings in vector store\")\n",
    "        \n",
    "        return chunks, embeddings\n",
    "    \n",
    "    def _chunk_text(self, text):\n",
    "        \"\"\"YOUR APPLICATION handles text chunking\"\"\"\n",
    "        words = text.split()\n",
    "        chunk_size = 300\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def process_query(self, user_query):\n",
    "        \"\"\"\n",
    "        Complete breakdown of who does what for each query\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== PROCESSING QUERY: '{user_query}' ===\")\n",
    "        \n",
    "        # STEP 1: YOUR APPLICATION vectorizes the query\n",
    "        print(\"\\n--- Step 1: Query Vectorization (Your App) ---\")\n",
    "        query_embedding = self.embedding_model.encode([user_query])\n",
    "        print(f\"✓ Your App: Converted query to embedding (shape: {query_embedding.shape})\")\n",
    "        print(f\"✓ Your App: Used same HuggingFace model as for book\")\n",
    "        \n",
    "        # STEP 2: YOUR APPLICATION searches vector store\n",
    "        print(\"\\n--- Step 2: Vector Search (Your App) ---\")\n",
    "        similarities = cosine_similarity(query_embedding, self.book_embeddings)[0]\n",
    "        top_indices = np.argsort(similarities)[-3:][::-1]  # Top 3\n",
    "        \n",
    "        print(f\"✓ Your App: Calculated similarities with {len(self.book_embeddings)} book chunks\")\n",
    "        print(f\"✓ Your App: Found top 3 most similar chunks\")\n",
    "        \n",
    "        # STEP 3: YOUR APPLICATION retrieves and converts back to text\n",
    "        print(\"\\n--- Step 3: Retrieval & Text Conversion (Your App) ---\")\n",
    "        relevant_chunks = []\n",
    "        for idx in top_indices:\n",
    "            relevant_chunks.append({\n",
    "                'text': self.book_chunks[idx],  # This is already natural language!\n",
    "                'similarity': similarities[idx],\n",
    "                'chunk_id': idx\n",
    "            })\n",
    "        \n",
    "        print(\"✓ Your App: Retrieved chunks are ALREADY in natural language\")\n",
    "        print(\"✓ Your App: No conversion needed - chunks were stored as text\")\n",
    "        \n",
    "        for i, chunk in enumerate(relevant_chunks):\n",
    "            print(f\"  Chunk {i+1}: Similarity={chunk['similarity']:.3f}, \"\n",
    "                  f\"Text='{chunk['text'][:50]}...'\")\n",
    "        \n",
    "        # STEP 4: YOUR APPLICATION prepares context for Claude\n",
    "        print(\"\\n--- Step 4: Context Preparation (Your App) ---\")\n",
    "        context = \"\\n\\n\".join([chunk['text'] for chunk in relevant_chunks])\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following text from a book:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "        \n",
    "        print(f\"✓ Your App: Prepared context ({len(context)} characters)\")\n",
    "        print(f\"✓ Your App: Created prompt for Claude\")\n",
    "        \n",
    "        # STEP 5: CLAUDE handles text generation\n",
    "        print(\"\\n--- Step 5: Text Generation (Claude) ---\")\n",
    "        print(\"✓ Claude: Receives TEXT prompt (not embeddings)\")\n",
    "        print(\"✓ Claude: Processes natural language\")\n",
    "        print(\"✓ Claude: Generates natural language response\")\n",
    "        \n",
    "        response = self.claude.messages.create(\n",
    "            model=\"claude-3-sonnet-20240229\",\n",
    "            max_tokens=500,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        answer = response.content[0].text\n",
    "        print(f\"✓ Claude: Generated answer ({len(answer)} characters)\")\n",
    "        \n",
    "        # STEP 6: YOUR APPLICATION returns final result\n",
    "        print(\"\\n--- Step 6: Result Assembly (Your App) ---\")\n",
    "        result = {\n",
    "            'query': user_query,\n",
    "            'answer': answer,\n",
    "            'sources': relevant_chunks,\n",
    "            'context_used': context\n",
    "        }\n",
    "        \n",
    "        print(\"✓ Your App: Assembled final result\")\n",
    "        print(\"✓ Your App: Ready to display to user\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def explain_responsibilities(self):\n",
    "        \"\"\"\n",
    "        Clear breakdown of responsibilities\n",
    "        \"\"\"\n",
    "        return \"\"\"\n",
    "        WHO DOES WHAT IN RAG:\n",
    "        \n",
    "        === YOUR APPLICATION HANDLES: ===\n",
    "        ✓ Loading HuggingFace embedding model\n",
    "        ✓ Chunking book text into pieces\n",
    "        ✓ Converting book chunks to embeddings\n",
    "        ✓ Storing embeddings in vector database\n",
    "        ✓ Converting user query to embeddings\n",
    "        ✓ Searching vector database for similar embeddings\n",
    "        ✓ Retrieving relevant text chunks (already in natural language!)\n",
    "        ✓ Preparing context for Claude\n",
    "        ✓ Sending text prompt to Claude\n",
    "        ✓ Receiving Claude's response\n",
    "        ✓ Assembling final result\n",
    "        \n",
    "        === CLAUDE HANDLES: ===\n",
    "        ✓ Receiving text prompt\n",
    "        ✓ Understanding natural language\n",
    "        ✓ Generating natural language response\n",
    "        ✓ Reasoning about the provided context\n",
    "        \n",
    "        === KEY INSIGHT: ===\n",
    "        There's NO \"conversion back to natural language\" step!\n",
    "        Your book chunks are STORED as natural language text.\n",
    "        Embeddings are only used for similarity comparison.\n",
    "        \n",
    "        Flow: Text → Embeddings → Similarity Search → Text → Claude\n",
    "        \"\"\"\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample book content\n",
    "    book_text = \"\"\"\n",
    "    Machine learning is a subset of artificial intelligence that enables \n",
    "    computers to learn and improve from experience without being explicitly \n",
    "    programmed. It uses algorithms to identify patterns in data and make \n",
    "    predictions or decisions based on that information.\n",
    "    \n",
    "    There are three main types of machine learning: supervised learning, \n",
    "    unsupervised learning, and reinforcement learning. Supervised learning \n",
    "    uses labeled data to train models, while unsupervised learning finds \n",
    "    patterns in unlabeled data.\n",
    "    \n",
    "    Deep learning is a subset of machine learning that uses neural networks \n",
    "    with multiple layers to process complex data like images, text, and speech.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    rag = RAGPipelineBreakdown(book_text, \"your-anthropic-api-key\")\n",
    "    \n",
    "    # Process a query with detailed breakdown\n",
    "    result = rag.process_query(\"What is machine learning?\")\n",
    "    \n",
    "    print(f\"\\n=== FINAL RESULT ===\")\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Sources: {len(result['sources'])} chunks used\")\n",
    "    \n",
    "    # Explain responsibilities\n",
    "    print(rag.explain_responsibilities())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
