{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279fdc32",
   "metadata": {},
   "source": [
    "# **Sparse Layers:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef2fc50",
   "metadata": {},
   "source": [
    "Sparse layers are neural network layers that are designed to work efficiently with sparse data representations. \n",
    "\n",
    "In PyTorch's `torch.nn` module, sparse layers are primarily represented by the **Embedding layer** when used with the `sparse=True` parameter.\n",
    "\n",
    "**The key sparse layer in PyTorch is:**\n",
    "   - **`torch.nn.Embedding`** - When `sparse=True` is set, gradient w.r.t. weight matrix will be a sparse tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d009c7",
   "metadata": {},
   "source": [
    "#### **Working Mechanism of Sparse Layers:**\n",
    "\n",
    "1. **Sparse Gradients, Not Sparse Weights:**\n",
    "   - It's important to understand that $sparse=True$ in $nn.Embedding$ makes the gradient sparse, not the weight matrix. \n",
    "\n",
    "   - The weight matrix itself remains dense, but the gradients computed during backpropagation are stored in a sparse format.\n",
    "\n",
    "2. **Coordinate (COO) Format:**\n",
    "   - PyTorch implements sparse tensors using the so-called Coordinate format, or COO format, as one of the storage formats for implementing sparse tensors. \n",
    "\n",
    "   - In COO format, the specified elements are stored as tuples of element indices. \n",
    "\n",
    "   - This means that a sparse tensor is represented as a pair of dense tensors: a tensor of values and a 2D tensor of indices.\n",
    "\n",
    "3. **Embedding Layer Mechanism:**   \n",
    "**The embedding layer works as a lookup table:**  \n",
    "   - Given an input index, it returns the corresponding vector from the weight matrix\n",
    "\n",
    "   - During forward pass, only the embedding vectors corresponding to the input indices are accessed\n",
    "   \n",
    "   - During backward pass, only the gradients for the accessed embeddings are computed and stored sparsely\n",
    "\n",
    "4. **Semi-Structured Sparsity:**   \n",
    "PyTorch also supports semi-structured sparse layers where you can accelerate the linear layers in your model if the weights are already semi-structured sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eccdf5",
   "metadata": {},
   "source": [
    "### **Main Logic of Using Sparse Layers in PyTorch:**\n",
    "\n",
    "**1. Memory Efficiency:**  \n",
    "\n",
    "   - **Gradient Storage**: Instead of storing gradients for all embedding vectors, sparse layers only store gradients for the embedding vectors that were actually accessed during the forward pass\n",
    "\n",
    "   - **Reduced Memory Footprint**: This is particularly beneficial when dealing with large vocabularies where only a small subset of embeddings are used in each batch\n",
    "\n",
    "**2. Computational Efficiency:**\n",
    "   - **Faster Updates**: Only the embeddings that were accessed need to be updated during the optimization step\n",
    "\n",
    "   - **Reduced Computation**: Less computation is required for gradient updates since many gradients are zero\n",
    "\n",
    "**3. Use Cases:**\n",
    "Sparse layers are particularly useful in:\n",
    "   - **Natural Language Processing**: Large vocabulary embeddings where each batch only uses a small fraction of the vocabulary\n",
    "\n",
    "   - **Recommender Systems**: In this example, we make a_user_id and b_user_id sparse since both have high cardinality\n",
    "\n",
    "   - **High-Cardinality Categorical Features**: When dealing with categorical features that have many possible values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eacb90",
   "metadata": {},
   "source": [
    "**4. Optimizer Compatibility:**   \n",
    "There's an important limitation: only a limited number of optimizers support sparse gradients: currently it's `optim.SGD`. This means we need to use compatible optimizers like `SGD` when working with sparse embeddings.\n",
    "\n",
    "**5. Performance Considerations:**   \n",
    "The performance benefit depends on the sparsity pattern and hardware. In some cases, sparse operations might be slower than dense operations, especially when the sparsity is not high enough to offset the overhead of sparse tensor operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced38382",
   "metadata": {},
   "source": [
    "#### **Example Usage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee3f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a sparse embedding layer\n",
    "embedding = nn.Embedding(num_embeddings=10000, \n",
    "                        embedding_dim=128, \n",
    "                        sparse=True)\n",
    "\n",
    "# Use with sparse-compatible optimizer\n",
    "optimizer = torch.optim.SGD(embedding.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6738d",
   "metadata": {},
   "source": [
    "The main advantage of using sparse layers is memory and computational efficiency when dealing with high-dimensional, sparse data where only a small fraction of the parameters are actively used in each training step."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
