{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e70780",
   "metadata": {},
   "source": [
    "# **Meaning: Embeddings:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff14df4",
   "metadata": {},
   "source": [
    "**The Basic Concept:**\n",
    "\n",
    "Imagine we're teaching a computer to understand words. Computers only understand numbers, not words like `\"cat,\"` `\"dog,\"` or `\"happiness.\"` So how do we convert words into numbers that computers can work with? \n",
    "\n",
    "> *`Embeddings are a way to represent words (or any categorical data) as vectors of numbers that capture their meaning and relationships.`*\n",
    "\n",
    "#### **Why Not Just Use Simple Numbers?**\n",
    "\n",
    "**Let's say we have these words:**\n",
    "   - $cat → 1$\n",
    "   - $dog → 2$  \n",
    "   - $king → 3$\n",
    "   - $queen → 4$\n",
    "\n",
    "**This approach has a big problem:** the computer thinks `\"dog\"` (2) is twice as important as `\"cat\"` (1), and `\"queen\"` (4) is twice as important as `\"dog\"` (2). \n",
    "\n",
    "But that's not how language works!\n",
    "\n",
    "#### **The Embedding Solution:**\n",
    "\n",
    "Instead of single numbers, embeddings represent each word as a **`vector`** (a list of numbers). \n",
    "\n",
    "**For example:**\n",
    "\n",
    "```raw\n",
    "    cat   → [0.2, -0.1, 0.8, 0.3]\n",
    "    dog   → [0.3, -0.2, 0.7, 0.4]\n",
    "    king  → [0.1, 0.9, -0.2, 0.6]\n",
    "    queen → [0.2, 0.8, -0.1, 0.5]\n",
    "```\n",
    "\n",
    "Notice how `\"cat\"` and `\"dog\"` have similar vectors (both pets), and `\"king\"` and `\"queen\"` have similar vectors (both royalty).\n",
    "\n",
    "### **How Embeddings Work:**\n",
    "\n",
    "**1. The Lookup Table Concept:**\n",
    "\n",
    "Think of embeddings as a giant lookup table:\n",
    "\n",
    "```raw \n",
    "    Word ID | Word   | Embedding Vector\n",
    "    --------|--------|----------------------\n",
    "    0       | cat    | [0.2, -0.1, 0.8, 0.3]\n",
    "    1       | dog    | [0.3, -0.2, 0.7, 0.4]\n",
    "    2       | king   | [0.1, 0.9, -0.2, 0.6]\n",
    "    3       | queen  | [0.2, 0.8, -0.1, 0.5]\n",
    "```\n",
    "\n",
    "**2. The Process:**\n",
    "\n",
    "   1. **`Input`**: You give the computer a word ID (like 0 for \"cat\")\n",
    "   2. **`Lookup`**: The computer looks up row 0 in the embedding table\n",
    "   3. **`Output`**: It returns the vector `[0.2, -0.1, 0.8, 0.3]`\n",
    "\n",
    "**3. Learning Meaningful Representations:**\n",
    "\n",
    "Initially, these vectors are random. But as the neural network trains on tasks (like predicting the next word in a sentence), it learns to adjust these vectors so that:\n",
    "   - Similar words have similar vectors\n",
    "   - The relationships between words are captured mathematically\n",
    "\n",
    "### **Real-World Example**\n",
    "\n",
    "Let's say you're building a movie recommendation system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5db9025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Titanic', 'Avatar', 'The Matrix', 'Toy Story']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = [\"Titanic\", \"Avatar\", \"The Matrix\", \"Toy Story\"]\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e88b4a",
   "metadata": {},
   "source": [
    "**Instead of representing movies as 1, 2, 3, 4, you create embeddings:**\n",
    "\n",
    "```raw \n",
    "   Titanic    → [0.8, 0.1, 0.2]  # [romance, sci-fi, animation]\n",
    "   Avatar     → [0.2, 0.9, 0.1]  # [romance, sci-fi, animation]\n",
    "   The Matrix → [0.1, 0.8, 0.0]  # [romance, sci-fi, animation]\n",
    "   Toy Story  → [0.0, 0.1, 0.9]  # [romance, sci-fi, animation]\n",
    "```\n",
    "\n",
    "Now the computer can understand that `\"Avatar\"` and `\"The Matrix\"` are both sci-fi movies!\n",
    "\n",
    "### **Why PyTorch Connects Sparsity with Embeddings:**\n",
    "\n",
    "**1. The Memory Problem:**\n",
    "\n",
    "Let's understand this with a concrete example:\n",
    "\n",
    "Imagine you're building a language model for English. You have:\n",
    "   - **Vocabulary size**: 50,000 words\n",
    "   - **Embedding dimension**: 300\n",
    "   - **Embedding table size**: 50,000 × 300 = 15,000,000 parameters\n",
    "\n",
    "That's 15 million numbers to store just for word embeddings!\n",
    "\n",
    "**2. The Sparsity Connection:**\n",
    "\n",
    "Here's the key insight: **In any given training batch, you only use a tiny fraction of all possible words.**\n",
    "\n",
    "For example, in a batch of 32 sentences:\n",
    "   - We might only encounter 500 different words out of 50,000\n",
    "   - That means we only need to update 500 embedding vectors, not all 50,000\n",
    "\n",
    "### **How Sparse Embeddings Work**\n",
    "\n",
    "**1. Traditional Dense Approach**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb077631",
   "metadata": {},
   "source": [
    "```python\n",
    "# Every training step updates ALL 50,000 embeddings\n",
    "# Even if only 500 words appeared in the batch\n",
    "# Gradients computed for all 15 million parameters\n",
    "```\n",
    "\n",
    "**2. Sparse Approach**:\n",
    "```python\n",
    "# Only update the 500 embeddings that were actually used\n",
    "# Gradients computed only for 500 × 300 = 150,000 parameters\n",
    "# 100x less computation!\n",
    "```\n",
    "\n",
    "### **The Technical Details:**\n",
    "\n",
    "When you set `sparse=True` in PyTorch's embedding layer:\n",
    "\n",
    "1. **Forward Pass**: Same as before - lookup the embedding vectors\n",
    "2. **Backward Pass**: Instead of computing gradients for all embeddings, only compute gradients for the embeddings that were actually accessed\n",
    "3. **Storage**: These gradients are stored in a sparse format (only non-zero gradients are stored)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8affb2",
   "metadata": {},
   "source": [
    "### **A Simple Analogy:**\n",
    "\n",
    "Think of it like a huge library with 50,000 books:\n",
    "\n",
    "**Dense Approach**: \n",
    "   - Every day, you check and potentially move every single book, even if only 10 people visited the library\n",
    "   - Very wasteful!\n",
    "\n",
    "**Sparse Approach**:\n",
    "   - You only check and move the books that were actually borrowed\n",
    "   - Much more efficient!\n",
    "\n",
    "### **Why This Matters:**\n",
    "\n",
    "1. **Memory Efficiency**: You save memory by not storing zero gradients\n",
    "2. **Speed**: You save computation by not calculating unnecessary gradients\n",
    "3. **Scalability**: You can handle much larger vocabularies (millions of words)\n",
    "\n",
    "**Code Example to Illustrate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bab24f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128])\n",
      "tensor([[   5, 1234, 8765]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 10000\n",
    "embed_dim = 128\n",
    "embedding = nn.Embedding(vocab_size, embed_dim, sparse=True)\n",
    "\n",
    "# Input: only 3 words out of 10,000 vocabulary\n",
    "word_ids = torch.tensor([5, 1234, 8765])  # 3 words\n",
    "\n",
    "# Forward pass - only accesses 3 embedding vectors\n",
    "output = embedding(word_ids)  # Shape: [3, 128]\n",
    "print(output.shape)  # Should print: torch.Size([3, 128])\n",
    "# Backward pass - only computes gradients for these 3 vectors\n",
    "# Not for all 10,000 vectors!\n",
    "loss = output.sum()\n",
    "loss.backward()\n",
    "\n",
    "# The gradient will be sparse - only 3 out of 10,000 rows have non-zero gradients\n",
    "\n",
    "# Prints the computed gradients: \n",
    "#print(embedding.weight.grad[word_ids])  # Shape: [3, 128]\n",
    "# When using sparse=True, embedding.weight.grad is a torch.sparse.FloatTensor. \n",
    "# Indexing it directly with word_ids may not work as expected.\n",
    "\n",
    "# Print the indices and values of non-zero gradients\n",
    "print(embedding.weight.grad._indices())  # Indices of updated rows   \n",
    "print(embedding.weight.grad._values())   # Corresponding gradients  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a87e62",
   "metadata": {},
   "source": [
    "### **The Bottom Line:**\n",
    "\n",
    "**PyTorch connects sparsity with embeddings because:**\n",
    "\n",
    "   1. **Embeddings are naturally sparse in usage** - you rarely use all possible categories at once\n",
    "   2. **This creates a massive optimization opportunity** - why waste computation on unused embeddings?\n",
    "   3. **The math works out perfectly** - sparse gradients are a natural fit for embedding layers\n",
    "\n",
    "This is why when we hear `\"sparse layers\"` in frameworks like PyTorch, it's almost always referring to embedding layers with `sparse=True` - it's the most common and beneficial use case for sparsity in deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90051362",
   "metadata": {},
   "source": [
    "----\n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35d3c9",
   "metadata": {},
   "source": [
    "## **Relevent Topics:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c89e2",
   "metadata": {},
   "source": [
    "**1. Tokenization (Text → Tokens):**\n",
    "\n",
    "> $\"Hello world!\" → [\"Hello\", \"world\", \"!\"] → [1234, 5678, 91011]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7653ed3",
   "metadata": {},
   "source": [
    "**2. Embedding (Tokens → Vectors):**  \n",
    "\n",
    "> $[1234, 5678, 91011] → [[0.1, 0.2, ...], [0.3, 0.4, ...], [0.5, 0.6, ...]]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71391968",
   "metadata": {},
   "source": [
    "#### **You Can't Just \"Plug In\" Your Vectors into Large Language Models:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d30d14",
   "metadata": {},
   "source": [
    "we cannot take embeddings from our custom model and directly use them with `Claude Sonnet 4` or any other pre-trained model. Here's why: \n",
    "\n",
    "**Vocabulary Mismatch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f47fbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your model's vocabulary\n",
    "your_vocab = {\"hello\": 0, \"world\": 1, \"book\": 2}\n",
    "\n",
    "# Claude's vocabulary (simplified)\n",
    "claude_vocab = {\"hello\": 1245, \"world\": 892, \"book\": 3421}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646d084",
   "metadata": {},
   "source": [
    "Even the same word `\"hello\"` has different token IDs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee942833",
   "metadata": {},
   "source": [
    "**Semantic Space Mismatch:** \n",
    "\n",
    "Even if vocabularies matched, the embedding spaces are completely different:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a55bd",
   "metadata": {},
   "source": [
    "```python\n",
    "# Your model learns: \"king\" - \"man\" + \"woman\" = \"queen\"\n",
    "your_embeddings[\"king\"] = [0.1, 0.2, 0.3]\n",
    "your_embeddings[\"queen\"] = [0.2, 0.3, 0.4]\n",
    "\n",
    "# Claude learns different relationships in different space\n",
    "claude_embeddings[\"king\"] = [0.8, -0.1, 0.5]\n",
    "claude_embeddings[\"queen\"] = [0.7, -0.2, 0.6]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256b4ba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
